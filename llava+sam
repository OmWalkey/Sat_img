import streamlit as st
from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, ImageFont
import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig
import numpy as np
import gc
import json
import datetime
import io
import base64
from typing import Dict, List, Tuple, Optional
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from collections import Counter
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator
import urllib.request
import os

# ---------------------- Streamlit Config ----------------------
st.set_page_config(page_title="Enhanced Satellite AI Pro with SAM2", layout="wide", page_icon="ğŸ›°ï¸")

# ---------------------- Model & Quantization Config ----------------------
MODEL_NAME = "llava-hf/llava-1.5-7b-hf"

# Improved BitsAndBytesConfig
BQB_CONFIG = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

@st.cache_resource
def load_llava_model():
    """Load LLaVA model with improved error handling and configuration"""
    try:
        # Load processor first
        processor = AutoProcessor.from_pretrained(MODEL_NAME)
        
        # Ensure processor has the correct configuration
        if not hasattr(processor, 'image_processor'):
            st.error("Image processor not found in the model. Please check model compatibility.")
            return None, None
        
        # Load model with proper configuration
        model = LlavaForConditionalGeneration.from_pretrained(
            MODEL_NAME,
            quantization_config=BQB_CONFIG,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # Ensure model is in eval mode
        model.eval()
        
        return model, processor
    except Exception as e:
        st.error(f"Error loading LLaVA model: {str(e)}")
        return None, None

@st.cache_resource
def download_sam2_checkpoint():
    """Download SAM2 model checkpoint if not present"""
    checkpoint_path = "sam2_hiera_large.pt"
    config_path = "sam2_hiera_l.yaml"
    
    if not os.path.exists(checkpoint_path):
        try:
            st.info("Downloading SAM2 model checkpoint (this may take a few minutes)...")
            checkpoint_url = "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt"
            urllib.request.urlretrieve(checkpoint_url, checkpoint_path)
            st.success("SAM2 checkpoint downloaded successfully!")
        except Exception as e:
            st.error(f"Failed to download SAM2 checkpoint: {e}")
            return None, None
    
    return checkpoint_path, config_path

@st.cache_resource
def load_sam2_model():
    """Load SAM2 model for segmentation"""
    try:
        checkpoint_path, config_path = download_sam2_checkpoint()
        if checkpoint_path is None:
            return None, None
        
        # Build SAM2 model
        sam2_model = build_sam2(config_path, checkpoint_path, device="cuda" if torch.cuda.is_available() else "cpu")
        
        # Create predictor and automatic mask generator
        predictor = SAM2ImagePredictor(sam2_model)
        mask_generator = SAM2AutomaticMaskGenerator(
            model=sam2_model,
            points_per_side=32,
            pred_iou_thresh=0.7,
            stability_score_thresh=0.85,
            crop_n_layers=1,
            crop_n_points_downscale_factor=2,
            min_mask_region_area=100,
        )
        
        return predictor, mask_generator
    except Exception as e:
        st.error(f"Error loading SAM2 model: {str(e)}")
        return None, None

# Initialize models
@st.cache_resource
def initialize_models():
    try:
        torch_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        llava_model, processor = load_llava_model()
        sam2_predictor, sam2_mask_generator = load_sam2_model()
        
        if llava_model is None or processor is None:
            return None, None, None, None, None
        
        # Move LLaVA model to device if not already done by device_map
        if torch_device.type == "cpu":
            llava_model = llava_model.to(torch_device)
        
        return llava_model, processor, sam2_predictor, sam2_mask_generator, torch_device
    except Exception as e:
        st.error(f"Error initializing models: {str(e)}")
        return None, None, None, None, None

llava_model, processor, sam2_predictor, sam2_mask_generator, torch_device = initialize_models()

# ---------------------- SAM2 Segmentation Functions ----------------------
def run_automatic_segmentation(image, mask_generator=None):
    """Run automatic segmentation on the image using SAM2"""
    if mask_generator is None:
        return None, "SAM2 mask generator not loaded"
    
    try:
        # Convert PIL image to numpy array
        image_np = np.array(image)
        
        # Generate masks
        masks = mask_generator.generate(image_np)
        
        # Sort masks by area (largest first)
        masks = sorted(masks, key=lambda x: x['area'], reverse=True)
        
        return masks, None
    
    except Exception as e:
        return None, f"Segmentation failed: {str(e)}"

def run_interactive_segmentation(image, input_points, input_labels, predictor=None):
    """Run interactive segmentation with point prompts"""
    if predictor is None:
        return None, "SAM2 predictor not loaded"
    
    try:
        # Convert PIL image to numpy array
        image_np = np.array(image)
        
        # Set image in predictor
        predictor.set_image(image_np)
        
        # Convert points and labels to numpy arrays
        input_points = np.array(input_points)
        input_labels = np.array(input_labels)
        
        # Predict masks
        masks, scores, logits = predictor.predict(
            point_coords=input_points,
            point_labels=input_labels,
            multimask_output=True,
        )
        
        # Return best mask (highest score)
        best_mask_idx = np.argmax(scores)
        return masks[best_mask_idx], scores[best_mask_idx]
    
    except Exception as e:
        return None, f"Interactive segmentation failed: {str(e)}"

def visualize_masks(image, masks, alpha=0.7):
    """Visualize segmentation masks on the image"""
    if not masks:
        return image
    
    # Create a copy of the image
    img_array = np.array(image)
    
    # Create an overlay for all masks
    overlay = np.zeros_like(img_array)
    
    # Color palette for different masks
    colors = [
        [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0], [255, 0, 255], [0, 255, 255],
        [128, 0, 0], [0, 128, 0], [0, 0, 128], [128, 128, 0], [128, 0, 128], [0, 128, 128],
        [192, 192, 192], [128, 128, 128], [255, 165, 0], [255, 20, 147], [0, 191, 255], [154, 205, 50]
    ]
    
    for i, mask_data in enumerate(masks):
        if isinstance(mask_data, dict) and 'segmentation' in mask_data:
            mask = mask_data['segmentation']
        else:
            mask = mask_data
        
        # Get color for this mask
        color = colors[i % len(colors)]
        
        # Apply color to mask area
        for c in range(3):
            overlay[mask, c] = color[c]
    
    # Blend original image with overlay
    result = cv2.addWeighted(img_array, 1-alpha, overlay, alpha, 0)
    
    return Image.fromarray(result)

def create_segmentation_statistics(masks):
    """Create statistics from segmentation results"""
    if not masks:
        return {}
    
    # Calculate statistics
    areas = []
    stability_scores = []
    predicted_ious = []
    
    for mask_data in masks:
        if isinstance(mask_data, dict):
            areas.append(mask_data.get('area', 0))
            stability_scores.append(mask_data.get('stability_score', 0))
            predicted_ious.append(mask_data.get('predicted_iou', 0))
    
    return {
        'total_segments': len(masks),
        'total_area_covered': sum(areas),
        'average_segment_area': np.mean(areas) if areas else 0,
        'median_segment_area': np.median(areas) if areas else 0,
        'average_stability_score': np.mean(stability_scores) if stability_scores else 0,
        'average_predicted_iou': np.mean(predicted_ious) if predicted_ious else 0,
        'largest_segment_area': max(areas) if areas else 0,
        'smallest_segment_area': min(areas) if areas else 0
    }

def analyze_mask_properties(masks, image_size):
    """Analyze properties of segmented regions"""
    if not masks:
        return []
    
    analysis_results = []
    total_image_area = image_size[0] * image_size[1]
    
    for i, mask_data in enumerate(masks):
        if isinstance(mask_data, dict) and 'segmentation' in mask_data:
            mask = mask_data['segmentation']
            area = mask_data.get('area', np.sum(mask))
            bbox = mask_data.get('bbox', [0, 0, 0, 0])  # [x, y, w, h]
            
            # Calculate additional properties
            area_percentage = (area / total_image_area) * 100
            aspect_ratio = bbox[2] / bbox[3] if bbox[3] > 0 else 1
            
            # Determine shape characteristics
            if aspect_ratio > 3:
                shape_type = "elongated"
            elif aspect_ratio < 0.33:
                shape_type = "tall"
            elif 0.8 <= aspect_ratio <= 1.2:
                shape_type = "square-like"
            else:
                shape_type = "rectangular"
            
            # Size classification
            if area_percentage > 10:
                size_class = "large"
            elif area_percentage > 1:
                size_class = "medium"
            else:
                size_class = "small"
            
            analysis_results.append({
                'segment_id': i + 1,
                'area': area,
                'area_percentage': area_percentage,
                'bbox': bbox,
                'aspect_ratio': aspect_ratio,
                'shape_type': shape_type,
                'size_class': size_class,
                'stability_score': mask_data.get('stability_score', 0),
                'predicted_iou': mask_data.get('predicted_iou', 0)
            })
    
    return analysis_results

# ---------------------- Image Processing Functions ----------------------
def process_image(uploaded_file):
    """Process uploaded image with error handling"""
    try:
        image = Image.open(uploaded_file)
        
        # Ensure RGB mode
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Resize if too large (but keep reasonable resolution for SAM2)
        max_size = 1024
        if max(image.size) > max_size:
            ratio = max_size / max(image.size)
            new_size = tuple(int(dim * ratio) for dim in image.size)
            image = image.resize(new_size, Image.Resampling.LANCZOS)
        
        return image
    except Exception as e:
        st.error(f"Error processing image: {str(e)}")
        return None

def enhance_satellite_image(image, contrast=1.2, brightness=1.0, saturation=1.1, sharpness=1.0, 
                          edge_enhance=False, denoise=False):
    """Enhanced preprocessing specifically for satellite imagery"""
    try:
        img = image.copy()
        
        # Basic enhancements
        if contrast != 1.0:
            img = ImageEnhance.Contrast(img).enhance(contrast)
        if brightness != 1.0:
            img = ImageEnhance.Brightness(img).enhance(brightness)
        if saturation != 1.0:
            img = ImageEnhance.Color(img).enhance(saturation)
        if sharpness != 1.0:
            img = ImageEnhance.Sharpness(img).enhance(sharpness)
        
        # Advanced processing
        if edge_enhance:
            img = img.filter(ImageFilter.EDGE_ENHANCE_MORE)
        if denoise:
            img = img.filter(ImageFilter.MedianFilter(size=3))
        
        return img
    except Exception as e:
        st.error(f"Error enhancing image: {str(e)}")
        return image

# ---------------------- LLaVA Analysis Function ----------------------
def run_analysis(image, prompt_text, model, processor, device, max_tokens=300, temperature=0.3, top_p=0.85):
    """Run satellite image analysis with improved error handling and proper input formatting"""
    
    if model is None or processor is None:
        return "Model not properly loaded. Please check the model initialization."
    
    try:
        # Prepare the conversation format that LLaVA expects
        conversation = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"You are an expert satellite image analyst. {prompt_text}"
                    },
                    {
                        "type": "image"
                    }
                ]
            }
        ]
        
        # Apply chat template
        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
        
        # Process inputs with proper formatting
        inputs = processor(
            text=prompt,
            images=image,
            return_tensors="pt",
            padding=True
        )
        
        # Move inputs to device
        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
        
        # Generation parameters
        gen_kwargs = {
            "max_new_tokens": max_tokens,
            "do_sample": True,
            "temperature": temperature,
            "top_p": top_p,
            "repetition_penalty": 1.1,
            "use_cache": True,
            "pad_token_id": processor.tokenizer.pad_token_id,
            "eos_token_id": processor.tokenizer.eos_token_id,
        }
        
        # Generate response with error handling
        with torch.no_grad():
            outputs = model.generate(**inputs, **gen_kwargs)
        
        # Decode response
        input_len = inputs['input_ids'].shape[1]
        gen_tokens = outputs[0][input_len:]
        result = processor.decode(gen_tokens, skip_special_tokens=True).strip()
        
        # Clean up GPU memory
        del inputs, outputs, gen_tokens
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        return result if result else "No response generated. Please try again with a different prompt."
        
    except Exception as e:
        error_msg = str(e)
        
        # Provide specific error guidance
        if "image tokens" in error_msg.lower():
            return "Error: Image token processing issue. Try resizing your image or using a different image format."
        elif "out of memory" in error_msg.lower():
            return "Error: GPU memory insufficient. Try reducing max_tokens or using CPU mode."
        elif "cuda" in error_msg.lower():
            return "Error: CUDA/GPU issue. Try switching to CPU mode in the settings."
        else:
            return f"Analysis failed: {error_msg}"

# ---------------------- Enhanced Prompts ----------------------
def get_enhanced_prompts():
    return {
        "Comprehensive Scene Description": """Provide a detailed, systematic description of this satellite image scene:

SCENE OVERVIEW:
- Overall scene type (urban, rural, industrial, natural, mixed)
- Dominant landscape characteristics
- General layout and organization patterns

BUILT ENVIRONMENT:
- Building types, sizes, and patterns visible
- Road networks and transportation infrastructure
- Infrastructure density and development patterns

NATURAL FEATURES:
- Vegetation types and coverage
- Water bodies and terrain features
- Land use patterns

Organize your response clearly and be specific about what you can observe.""",

        "Infrastructure Analysis": """Analyze the infrastructure visible in this satellite image:

TRANSPORTATION:
- Road networks, intersections, and connectivity
- Parking areas and vehicle infrastructure

BUILDINGS:
- Building types, sizes, and construction patterns
- Density and spatial arrangement

UTILITIES:
- Power lines, communication towers, or utility infrastructure if visible

Focus only on clearly visible infrastructure elements.""",

        "Environmental Assessment": """Assess the environmental features in this satellite image:

VEGETATION:
- Types of vegetation visible
- Coverage density and health indicators
- Seasonal conditions if apparent

LAND USE:
- Agricultural areas, urban development, natural areas
- Land cover patterns and boundaries

WATER FEATURES:
- Rivers, lakes, ponds, or water bodies
- Drainage patterns if visible

Report only what is clearly observable in the image.""",

        "Land Use Classification": """Classify the land use patterns in this satellite image:

PRIMARY LAND USES:
- Residential areas and housing patterns
- Commercial/industrial zones
- Agricultural or undeveloped land
- Transportation infrastructure

DEVELOPMENT PATTERNS:
- Density gradients and spatial organization
- Mixed-use areas if present
- Boundary definitions between different uses

Provide specific observations about land use distribution.""",

        "Segmentation Analysis": """Based on the segmented regions in this satellite image, provide analysis of:

SEGMENTED REGIONS:
- Types and characteristics of segmented areas
- Spatial patterns and relationships between segments
- Boundary definitions and transitions

LAND COVER ANALYSIS:
- Different surface types and materials
- Vegetation vs built environment segments
- Water bodies and natural features

SPATIAL ORGANIZATION:
- How different segments relate to each other
- Development patterns and land use efficiency
- Infrastructure connectivity between segments

Focus on interpreting the segmented regions within the broader context of satellite imagery analysis."""
    }

# ---------------------- Sidebar Configuration ----------------------
st.sidebar.header("ğŸ“¥ Input & Settings")

# File upload
uploaded = st.sidebar.file_uploader(
    "Upload a satellite image", 
    type=["png", "jpg", "jpeg", "tif", "tiff"], 
    accept_multiple_files=False
)

# Analysis mode selection
analysis_mode = st.sidebar.radio(
    "Analysis Mode:",
    ["ğŸ” Scene Analysis (LLaVA)", "ğŸ¯ Segmentation (SAM2)", "ğŸ”¬ Combined Analysis"],
    index=2
)

# Enhanced image preprocessing
with st.sidebar.expander("ğŸ–¼ï¸ Image Processing", expanded=False):
    contrast = st.slider("Contrast", 0.5, 3.0, 1.2, step=0.1)
    brightness = st.slider("Brightness", 0.5, 2.0, 1.0, step=0.1)
    saturation = st.slider("Saturation", 0.0, 2.0, 1.1, step=0.1)
    sharpness = st.slider("Sharpness", 0.0, 3.0, 1.2, step=0.1)
    edge_enhance = st.checkbox("Edge Enhancement", value=False)
    denoise = st.checkbox("Noise Reduction", value=False)

# SAM2 Segmentation Settings
if analysis_mode in ["ğŸ¯ Segmentation (SAM2)", "ğŸ”¬ Combined Analysis"]:
    with st.sidebar.expander("ğŸ¯ SAM2 Segmentation Settings", expanded=True):
        segmentation_mode = st.selectbox(
            "Segmentation Mode:",
            ["Automatic", "Interactive (Click Points)"]
        )
        mask_alpha = st.slider("Mask Transparency", 0.1, 1.0, 0.7, step=0.1)
        show_statistics = st.checkbox("Show Segmentation Statistics", value=True)
        max_masks_display = st.slider("Max Masks to Display", 5, 50, 20, step=5)

# LLaVA Analysis configuration
if analysis_mode in ["ğŸ” Scene Analysis (LLaVA)", "ğŸ”¬ Combined Analysis"]:
    enhanced_prompts = get_enhanced_prompts()
    analysis_type = st.sidebar.selectbox(
        "Analysis Type:", 
        list(enhanced_prompts.keys()) + ["Custom"]
    )

    if analysis_type == "Custom":
        prompt = st.sidebar.text_area(
            "Custom Prompt:",
            "Describe what you see in this satellite image in detail.",
            height=100
        )
    else:
        prompt = enhanced_prompts[analysis_type]

    # Model parameters
    with st.sidebar.expander("âš™ï¸ LLaVA Model Settings", expanded=False):
        max_tokens = st.slider("Max Response Tokens", 150, 500, 300, step=50)
        temperature = st.slider("Temperature", 0.1, 1.0, 0.3, step=0.1)
        top_p = st.slider("Top-p", 0.1, 1.0, 0.85, step=0.05)

# ---------------------- Main Interface ----------------------
st.title("ğŸ›°ï¸ Enhanced Satellite AI Analysis with SAM2")
st.markdown("**Advanced satellite and aerial image analysis with LLaVA + SAM2 Segmentation**")

# Model status indicators
col_status1, col_status2 = st.columns(2)
with col_status1:
    if llava_model is None or processor is None:
        st.error("âš ï¸ LLaVA Model failed to load")
    else:
        st.success("âœ… LLaVA Model loaded")

with col_status2:
    if sam2_predictor is None or sam2_mask_generator is None:
        st.error("âš ï¸ SAM2 Model failed to load")
    else:
        st.success("âœ… SAM2 Model loaded")

# Main content area
if uploaded:
    image = process_image(uploaded)
    if image:
        # Apply enhancements
        enhanced_image = enhance_satellite_image(
            image, contrast, brightness, saturation, sharpness, edge_enhance, denoise
        )
        
        # Create tabs for different views
        tab1, tab2, tab3 = st.tabs(["ğŸ“· Original Image", "ğŸ¯ Segmentation", "ğŸ“Š Results"])
        
        with tab1:
            st.subheader("Enhanced Image")
            st.image(enhanced_image, caption=f"Enhanced: {uploaded.name}", use_column_width=True)
            st.info(f"Image size: {enhanced_image.size[0]} x {enhanced_image.size[1]} pixels")
        
        with tab2:
            if analysis_mode in ["ğŸ¯ Segmentation (SAM2)", "ğŸ”¬ Combined Analysis"]:
                st.subheader("SAM2 Segmentation")
                
                if segmentation_mode == "Automatic":
                    if st.button("ğŸš€ Run Automatic Segmentation", type="primary"):
                        if sam2_mask_generator is not None:
                            with st.spinner("Segmenting image..."):
                                masks, error = run_automatic_segmentation(
                                    enhanced_image, 
                                    mask_generator=sam2_mask_generator
                                )
                            
                            if error:
                                st.error(f"Segmentation failed: {error}")
                            else:
                                if masks:
                                    # Limit number of masks for display
                                    display_masks = masks[:max_masks_display]
                                    
                                    # Visualize masks
                                    segmented_image = visualize_masks(enhanced_image, display_masks, mask_alpha)
                                    st.image(segmented_image, caption=f"Segmented: {len(display_masks)} regions shown", use_column_width=True)
                                    
                                    # Store results in session state
                                    st.session_state.current_masks = masks
                                    st.session_state.segmentation_stats = create_segmentation_statistics(masks)
                                    st.session_state.mask_properties = analyze_mask_properties(masks, enhanced_image.size)
                                    
                                    st.success(f"âœ… Generated {len(masks)} segments (showing top {len(display_masks)})!")
                                else:
                                    st.warning("No segments generated.")
                        else:
                            st.error("SAM2 model not available.")
                
                elif segmentation_mode == "Interactive (Click Points)":
                    st.info("Interactive segmentation: Click on the image to add points, then run segmentation.")
                    st.write("Note: Interactive mode requires additional implementation for point selection in Streamlit.")
                    # Interactive mode would require additional UI components for point selection
                    # This is a simplified placeholder
                    
            else:
                st.info("Segmentation not enabled in current analysis mode.")
        
        with tab3:
            st.subheader("Analysis Results")
            
            # SAM2 Segmentation Statistics
            if (analysis_mode in ["ğŸ¯ Segmentation (SAM2)", "ğŸ”¬ Combined Analysis"] and 
                hasattr(st.session_state, 'current_masks') and 
                st.session_state.current_masks and show_statistics):
                
                st.write("### ğŸ¯ Segmentation Statistics")
                stats = st.session_state.segmentation_stats
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Total Segments", stats['total_segments'])
                with col2:
                    st.metric("Avg Area", f"{stats['average_segment_area']:.0f}")
                with col3:
                    st.metric("Avg Stability", f"{stats['average_stability_score']:.3f}")
                with col4:
                    st.metric("Avg IoU", f"{stats['average_predicted_iou']:.3f}")
                
                # Segment size distribution chart
                if st.session_state.mask_properties:
                    areas = [prop['area'] for prop in st.session_state.mask_properties]
                    fig = px.histogram(
                        x=areas,
                        nbins=20,
                        title="Segment Size Distribution",
                        labels={'x': 'Segment Area (pixels)', 'y': 'Count'}
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # Shape analysis
                if st.session_state.mask_properties:
                    shape_counts = Counter([prop['shape_type'] for prop in st.session_state.mask_properties])
                    fig = px.pie(
                        values=list(shape_counts.values()),
                        names=list(shape_counts.keys()),
                        title="Shape Type Distribution"
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                # Detailed segmentation table
                if st.session_state.mask_properties:
                    seg_df = pd.DataFrame(st.session_state.mask_properties)
                    st.write("### Segment Analysis Details")
                    st.dataframe(seg_df.round(3))
            
            # LLaVA Scene Analysis
            if analysis_mode in ["ğŸ” Scene Analysis (LLaVA)", "ğŸ”¬ Combined Analysis"]:
                st.write("### ğŸ” Scene Analysis")
                
                # Modify prompt for combined analysis
                if (analysis_mode == "ğŸ”¬ Combined Analysis" and 
                    hasattr(st.session_state, 'current_masks') and 
                    st.session_state.current_masks):
                    
                    num_segments = len(st.session_state.current_masks)
                    avg_area = st.session_state.segmentation_stats['average_segment_area']
                    segmentation_context = (f"\n\nNOTE: Image segmentation has identified {num_segments} distinct regions "
                                          f"with an average area of {avg_area:.0f} pixels. Please incorporate this "
                                          "segmentation information into your analysis, discussing how these regions "
                                          "relate to different land uses, infrastructure, or natural features.")
                    combined_prompt = prompt + segmentation_context
                else:
                    combined_prompt = prompt
                
                if st.button("ğŸš€ Run Scene Analysis", type="primary"):
                    if llava_model is not None and processor is not None:
                        with st.spinner("Analyzing scene..."):
                            result = run_analysis(
                                enhanced_image, combined_prompt, llava_model, processor, torch_device,
                                max_tokens, temperature, top_p
                            )
                        
                        st.success("Scene Analysis Complete!")
                        st.write("**Analysis Result:**")
                        st.write(result)
                        
                        # Store result in session state
                        if 'analysis_results' not in st.session_state:
                            st.session_state.analysis_results = []
                        
                        analysis_data = {
                            'timestamp': datetime.datetime.now().isoformat(),
                            'image_name': uploaded.name,
                            'analysis_type': analysis_type if 'analysis_type' in locals() else 'Segmentation',
                            'result': result,
                            'image_size': enhanced_image.size,
                            'num_segments': len(st.session_state.current_masks) if hasattr(st.session_state, 'current_masks') else 0
                        }
                        
                        st.session_state.analysis_results.
append(analysis_data)
                        
                        # Optional: Export functionality
                        export_data = {
                            'analysis': analysis_data,
                            'segmentation_stats': st.session_state.segmentation_stats if hasattr(st.session_state, 'segmentation_stats') else None,
                            'mask_properties': st.session_state.mask_properties if hasattr(st.session_state, 'mask_properties') else None
                        }
                        
                        st.download_button(
                            label="ğŸ“¥ Download Analysis Report",
                            data=json.dumps(export_data, indent=2),
                            file_name=f"satellite_analysis_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                            mime="application/json"
                        )
                        
                    else:
                        st.error("LLaVA model not available. Please check model initialization.")

# ---------------------- Analysis History ----------------------
if hasattr(st.session_state, 'analysis_results') and st.session_state.analysis_results:
    with st.expander("ğŸ“š Analysis History", expanded=False):
        st.write(f"**{len(st.session_state.analysis_results)} previous analyses**")
        
        for i, analysis in enumerate(reversed(st.session_state.analysis_results[-5:])):  # Show last 5
            with st.container():
                st.write(f"**Analysis {len(st.session_state.analysis_results) - i}** - {analysis['timestamp'][:19]}")
                st.write(f"*Image:* {analysis['image_name']} | *Type:* {analysis['analysis_type']}")
                if analysis['num_segments'] > 0:
                    st.write(f"*Segments:* {analysis['num_segments']}")
                
                with st.expander(f"View Result {len(st.session_state.analysis_results) - i}", expanded=False):
                    st.write(analysis['result'])
                st.divider()

# ---------------------- Performance Monitoring ----------------------
with st.sidebar.expander("ğŸ”§ System Info", expanded=False):
    st.write("**GPU Available:**", "âœ… Yes" if torch.cuda.is_available() else "âŒ No")
    if torch.cuda.is_available():
        st.write("**GPU Memory:**", f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        st.write("**GPU Usage:**", f"{torch.cuda.memory_allocated() / 1e9:.2f} GB allocated")
    
    st.write("**Models Status:**")
    st.write("- LLaVA:", "âœ… Loaded" if llava_model is not None else "âŒ Failed")
    st.write("- SAM2:", "âœ… Loaded" if sam2_predictor is not None else "âŒ Failed")

# ---------------------- Footer ----------------------
st.sidebar.markdown("---")
st.sidebar.markdown("**ğŸ›°ï¸ Enhanced Satellite AI Pro**")
st.sidebar.markdown("Powered by LLaVA-1.5 + SAM2")
st.sidebar.markdown("Advanced satellite image analysis with AI segmentation")

# Memory cleanup button
if st.sidebar.button("ğŸ§¹ Clear Memory Cache"):
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    st.sidebar.success("Memory cache cleared!")

# Clear analysis history
if st.sidebar.button("ğŸ—‘ï¸ Clear Analysis History"):
    if hasattr(st.session_state, 'analysis_results'):
        del st.session_state.analysis_results
    if hasattr(st.session_state, 'current_masks'):
        del st.session_state.current_masks
    if hasattr(st.session_state, 'segmentation_stats'):
        del st.session_state.segmentation_stats
    if hasattr(st.session_state, 'mask_properties'):
        del st.session_state.mask_properties
    st.sidebar.success("Analysis history cleared!")

# ---------------------- Usage Instructions ----------------------
if not uploaded:
    st.markdown("## ğŸš€ Getting Started")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### ğŸ“‹ Instructions
        1. **Upload** a satellite or aerial image
        2. **Choose** your analysis mode:
           - ğŸ” Scene Analysis: AI-powered description
           - ğŸ¯ Segmentation: Object/region detection
           - ğŸ”¬ Combined: Both analyses together
        3. **Adjust** image processing settings if needed
        4. **Run** the analysis and view results
        """)
    
    with col2:
        st.markdown("""
        ### ğŸ¯ Best Practices
        - Use **high-resolution** satellite images
        - Try different **analysis types** for comprehensive insights
        - Adjust **image enhancement** for better results
        - Use **segmentation** to identify specific regions
        - **Export** results for further analysis
        """)
    
    st.markdown("### ğŸŒŸ Features")
    feature_col1, feature_col2, feature_col3 = st.columns(3)
    
    with feature_col1:
        st.markdown("""
        **ğŸ” Scene Analysis**
        - Infrastructure detection
        - Land use classification
        - Environmental assessment
        - Comprehensive descriptions
        """)
    
    with feature_col2:
        st.markdown("""
        **ğŸ¯ SAM2 Segmentation**
        - Automatic region detection
        - Interactive point-based selection
        - Detailed mask statistics
        - Shape and size analysis
        """)
    
    with feature_col3:
        st.markdown("""
        **ğŸ”¬ Combined Analysis**
        - Integrated AI insights
        - Segmentation-aware descriptions
        - Comprehensive reporting
        - Export capabilities
        """)

# ---------------------- Error Handling & Troubleshooting ----------------------
if uploaded and (llava_model is None or sam2_predictor is None):
    st.error("âš ï¸ Some models failed to load. Please check the following:")
    
    troubleshooting_steps = [
        "Ensure sufficient GPU/CPU memory is available",
        "Check internet connection for model downloads",
        "Verify CUDA installation if using GPU",
        "Try restarting the application",
        "Check that all required packages are installed"
    ]
    
    for i, step in enumerate(troubleshooting_steps, 1):
        st.write(f"{i}. {step}")
    
    if st.button("ğŸ”„ Retry Model Loading"):
        st.experimental_rerun()

# ---------------------- Advanced Configuration ----------------------
with st.sidebar.expander("ğŸ”¬ Advanced Settings", expanded=False):
    st.markdown("**Experimental Features**")
    
    # Multi-image comparison (placeholder)
    enable_comparison = st.checkbox("Enable Multi-Image Comparison", value=False)
    if enable_comparison:
        st.info("Multi-image comparison feature coming soon!")
    
    # Custom model paths (placeholder)
    custom_model_path = st.text_input("Custom Model Path (Optional)", placeholder="Leave empty for default")
    if custom_model_path:
        st.warning("Custom model loading not implemented yet.")
    
    # Performance optimization
    st.markdown("**Performance Settings**")
    low_memory_mode = st.checkbox("Low Memory Mode", value=False)
    if low_memory_mode:
        st.info("Low memory mode reduces model precision but uses less RAM.")
    
    # Debug mode
    debug_mode = st.checkbox("Debug Mode", value=False)
    if debug_mode:
        st.info("Debug mode provides additional logging information.")

# ---------------------- Version Information ----------------------
st.sidebar.markdown("---")
st.sidebar.markdown("**Version:** 2.0.0")
st.sidebar.markdown("**Last Updated:** June 2025")
st.sidebar.markdown("**Models:** LLaVA-1.5-7B + SAM2")

# Add some final cleanup
if __name__ == "__main__":
    # Ensure cleanup on script exit
    import atexit
    
    def cleanup():
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
    
    atexit.register(cleanup)
